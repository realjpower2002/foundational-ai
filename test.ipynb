{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of the current notebook to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'Foundational Models')))\n",
    "\n",
    "# Now you can import your module\n",
    "import mlp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2],\n",
       "       [ 1.5],\n",
       "       [ 2.2],\n",
       "       [-0.1],\n",
       "       [ 0.8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = mlp.Softmax\n",
    "\n",
    "data = np.array([0.2, 1.5, 2.2, -0.1, 0.8]).reshape(-1,1)\n",
    "\n",
    "data\n",
    "#s = softmax.forward(softmax, data)\n",
    "\n",
    "#s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00467766, -0.01716372, -0.03456349, -0.0034653 , -0.00852325],\n",
       "       [-0.01716372, -0.06297879, -0.1268237 , -0.0127152 , -0.03127434],\n",
       "       [-0.03456349, -0.1268237 , -0.25539157, -0.02560526, -0.06297879],\n",
       "       [-0.0034653 , -0.0127152 , -0.02560526, -0.00256715, -0.00631418],\n",
       "       [-0.00852325, -0.03127434, -0.06297879, -0.00631418, -0.01553038]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape\n",
    "\n",
    "# This (or something approximating it) is good for one sample.\n",
    "np.einsum(\"ij, jk -> ik\", s, -s.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2  0.1]\n",
      " [ 1.5 -0.9]\n",
      " [-0.1  2.1]\n",
      " [ 2.2 -0.3]\n",
      " [ 0.8 -0.9]]\n",
      "[17.45581167 10.82529837]\n",
      "Division : \n",
      " [[0.04690305 0.1020915 ]\n",
      " [0.25674481 0.03755736]\n",
      " [0.05183588 0.7543598 ]\n",
      " [0.51702056 0.06843398]\n",
      " [0.1274957  0.03755736]]\n",
      "Vectorwise vivision : \n",
      " [[0.04690305 0.1020915 ]\n",
      " [0.25674481 0.03755736]\n",
      " [0.05183588 0.7543598 ]\n",
      " [0.51702056 0.06843398]\n",
      " [0.1274957  0.03755736]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.04690305, 0.1020915 ],\n",
       "       [0.25674481, 0.03755736],\n",
       "       [0.05183588, 0.7543598 ],\n",
       "       [0.51702056, 0.06843398],\n",
       "       [0.1274957 , 0.03755736]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[-0.2, 1.5, -0.1, 2.2, 0.8],[0.1,-0.9,2.1,-0.3,-0.9]]).transpose()\n",
    "\n",
    "print(data)\n",
    "\n",
    "exps = np.exp(data)\n",
    "summed_exps = np.einsum(\"ij -> j\", exps)\n",
    "\n",
    "print(summed_exps)\n",
    "\n",
    "print(\"Division : \\n\",exps / summed_exps)\n",
    "\n",
    "print(\"Vectorwise vivision : \\n\",exps/summed_exps[:])\n",
    "\n",
    "softmax = mlp.Softmax\n",
    "\n",
    "softmax.forward(softmax, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "einstein sum subscripts string contains too many subscripts for operand 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbij, bj -> bi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Foundational Models/Multilayer Perceptron/.venv/lib/python3.10/site-packages/numpy/_core/einsumfunc.py:1423\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[1;32m   1422\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: einstein sum subscripts string contains too many subscripts for operand 0"
     ]
    }
   ],
   "source": [
    "np.einsum(\"bij, bj -> bi\", s, -s.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2  0.1]\n",
      " [ 1.5 -0.9]\n",
      " [-0.1  2.1]\n",
      " [ 2.2 -0.3]\n",
      " [ 0.8 -0.9]]\n",
      "[[0.04690305 0.1020915 ]\n",
      " [0.25674481 0.03755736]\n",
      " [0.05183588 0.7543598 ]\n",
      " [0.51702056 0.06843398]\n",
      " [0.1274957  0.03755736]]\n",
      "\n",
      "(5, 1, 2)\n",
      "(1, 5, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operand has more dimensions than subscripts given in einstein sum, but no '...' ellipsis provided to broadcast the extra dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(s_3D\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(s_3D_T\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 25\u001b[0m jacobian_stack \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbij,bj->bi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_3D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_3D_T\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m jacobian_stack\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Desktop/Foundational Models/Multilayer Perceptron/.venv/lib/python3.10/site-packages/numpy/_core/einsumfunc.py:1423\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[1;32m   1422\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: operand has more dimensions than subscripts given in einstein sum, but no '...' ellipsis provided to broadcast the extra dimensions."
     ]
    }
   ],
   "source": [
    "data\n",
    "\n",
    "# data = np.array([[-0.2, 1.5, -0.1, 2.2, 0.8],[0.1,-0.9,2.1,-0.3,-0.9]]).transpose()\n",
    "\n",
    "a = mlp.Softmax\n",
    "\n",
    "print(data)\n",
    "print(a.forward(a,data))\n",
    "\n",
    "s = a.forward(a,data)\n",
    "\n",
    "# rows and columns x columns and rows\n",
    "#\n",
    "# m x 1 x n  .  1 x m x n\n",
    "#\n",
    "#\n",
    "s_3D = s[:,:,np.newaxis].transpose(0,2,1)\n",
    "s_3D_T = s_3D.transpose(1,0,2)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(s_3D.shape)\n",
    "print(s_3D_T.shape)\n",
    "\n",
    "jacobian_stack = np.einsum(\"bij,bj->bi\", s_3D, s_3D_T)\n",
    "\n",
    "jacobian_stack.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
